{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20M Parameter Text Generation Model - Kaggle Training\n",
    "## Simple Version - Uses Pre-installed Packages\n",
    "\n",
    "This notebook trains a transformer model on Kaggle using only pre-installed packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fix Protobuf Issue (Run This First!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix protobuf version conflict\n",
    "!pip uninstall -y protobuf\n",
    "!pip install -q protobuf==3.20.3\n",
    "print(\"‚úì Protobuf fixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "print(\"‚úì All imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {__import__('transformers').__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU detected! Training will be very slow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 5e-4,\n",
    "    'epochs': 3,\n",
    "    'warmup_steps': 500,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'save_steps': 1000,\n",
    "    'eval_steps': 500,\n",
    "    'max_length': 512,\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key:30s}: {value}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture (10M parameters)\n",
    "model_config = GPT2Config(\n",
    "    vocab_size=50257,\n",
    "    n_positions=512,\n",
    "    n_embd=256,\n",
    "    n_layer=8,\n",
    "    n_head=8,\n",
    "    n_inner=1024,\n",
    "    activation_function='gelu_new',\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    attn_pdrop=0.1,\n",
    ")\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"  Vocabulary size: {model_config.vocab_size:,}\")\n",
    "print(f\"  Max sequence length: {model_config.n_positions}\")\n",
    "print(f\"  Embedding dimension: {model_config.n_embd}\")\n",
    "print(f\"  Number of layers: {model_config.n_layer}\")\n",
    "print(f\"  Number of heads: {model_config.n_head}\")\n",
    "print(f\"  FFN dimension: {model_config.n_inner}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model from scratch\n",
    "print(\"Initializing model...\")\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n‚úì Model initialized successfully\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: {total_params * 4 / 1e6:.2f} MB (FP32)\")\n",
    "print(f\"\\n‚úì Starting fresh training from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"‚úì Tokenizer loaded (vocab size: {len(tokenizer)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"\\nLoading dataset...\")\n",
    "dataset = load_dataset('wikitext', 'wikitext-103-v1')\n",
    "\n",
    "print(f\"‚úì Dataset loaded\")\n",
    "print(f\"  Train samples: {len(dataset['train']):,}\")\n",
    "print(f\"  Validation samples: {len(dataset['validation']):,}\")\n",
    "print(f\"  Test samples: {len(dataset['test']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=CONFIG['max_length'],\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "print(\"  This may take a few minutes...\")\n",
    "\n",
    "tokenized_train = dataset['train'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "\n",
    "tokenized_val = dataset['validation'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['validation'].column_names,\n",
    "    desc=\"Tokenizing validation\"\n",
    ")\n",
    "\n",
    "tokenized_train.set_format('torch')\n",
    "tokenized_val.set_format('torch')\n",
    "\n",
    "print(\"‚úì Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    tokenized_train,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_val,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"‚úì DataLoaders created\")\n",
    "print(f\"  Train batches: {len(train_loader):,}\")\n",
    "print(f\"  Val batches: {len(val_loader):,}\")\n",
    "print(f\"  Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * CONFIG['epochs'] // CONFIG['gradient_accumulation_steps']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=CONFIG['warmup_steps'],\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(\"‚úì Optimizer and scheduler configured\")\n",
    "print(f\"  Total training steps: {total_steps:,}\")\n",
    "print(f\"  Warmup steps: {CONFIG['warmup_steps']:,}\")\n",
    "print(f\"  Initial learning rate: {CONFIG['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=input_ids\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss / CONFIG['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        if (step + 1) % CONFIG['gradient_accumulation_steps'] == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * CONFIG['gradient_accumulation_steps']\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{loss.item() * CONFIG['gradient_accumulation_steps']:.4f}\",\n",
    "            'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "        })\n",
    "        \n",
    "        # Save checkpoint periodically\n",
    "        if (step + 1) % CONFIG['save_steps'] == 0:\n",
    "            checkpoint_path = f'/kaggle/working/checkpoint_epoch{epoch}_step{step+1}.pt'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'step': step + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': total_loss / (step + 1),\n",
    "                'config': CONFIG,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"\\n‚úì Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            total_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "print(\"‚úì Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "training_history = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch}/{CONFIG['epochs']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        epoch\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_perplexity = evaluate(model, val_loader, device)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch} Results:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Perplexity: {val_perplexity:.2f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Save history\n",
    "    training_history.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'val_perplexity': val_perplexity\n",
    "    })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_perplexity': val_perplexity,\n",
    "            'config': CONFIG,\n",
    "            'model_config': model_config.to_dict(),\n",
    "        }, '/kaggle/working/best_model.pt')\n",
    "        print(\"‚úì Saved best model\")\n",
    "    \n",
    "    # Save epoch checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'val_perplexity': val_perplexity,\n",
    "        'config': CONFIG,\n",
    "        'model_config': model_config.to_dict(),\n",
    "    }, f'/kaggle/working/checkpoint_epoch{epoch}.pt')\n",
    "    print(f\"‚úì Saved epoch {epoch} checkpoint\")\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "with open('/kaggle/working/training_history.json', 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "print(\"‚úì Training history saved!\")\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(\"=\" * 60)\n",
    "for entry in training_history:\n",
    "    print(f\"Epoch {entry['epoch']}: \"\n",
    "          f\"Train Loss={entry['train_loss']:.4f}, \"\n",
    "          f\"Val Loss={entry['val_loss']:.4f}, \"\n",
    "          f\"Perplexity={entry['val_perplexity']:.2f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Text Generation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100, temperature=0.8, num_return_sequences=1):\n",
    "    \"\"\"Generate text from a prompt\"\"\"\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return [tokenizer.decode(seq, skip_special_tokens=True) for seq in output]\n",
    "\n",
    "# Test generation\n",
    "test_prompts = [\n",
    "    \"The future of artificial intelligence\",\n",
    "    \"In a world where technology\",\n",
    "    \"Scientists have discovered\",\n",
    "    \"Once upon a time\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEXT GENERATION EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"-\" * 60)\n",
    "    generated = generate_text(prompt, max_length=150, temperature=0.8)\n",
    "    print(generated[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in HuggingFace format\n",
    "print(\"Saving final model...\")\n",
    "model.save_pretrained('/kaggle/working/final_model')\n",
    "tokenizer.save_pretrained('/kaggle/working/final_model')\n",
    "\n",
    "# Save config\n",
    "with open('/kaggle/working/final_model/training_config.json', 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì All models and configs saved!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nOutput files:\")\n",
    "print(\"  üìÅ /kaggle/working/\")\n",
    "print(\"    ‚îú‚îÄ‚îÄ best_model.pt (best checkpoint)\")\n",
    "print(\"    ‚îú‚îÄ‚îÄ checkpoint_epoch*.pt (epoch checkpoints)\")\n",
    "print(\"    ‚îú‚îÄ‚îÄ training_history.json (training metrics)\")\n",
    "print(\"    ‚îî‚îÄ‚îÄ final_model/ (HuggingFace format)\")\n",
    "print(\"        ‚îú‚îÄ‚îÄ pytorch_model.bin\")\n",
    "print(\"        ‚îú‚îÄ‚îÄ config.json\")\n",
    "print(\"        ‚îú‚îÄ‚îÄ training_config.json\")\n",
    "print(\"        ‚îî‚îÄ‚îÄ tokenizer files\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
