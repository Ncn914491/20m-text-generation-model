# Multi-GPU Training Guide for Kaggle

## ğŸš€ What's New

This notebook (`text_generation_kaggle_multigpu.ipynb`) is specifically optimized for **Kaggle's 2xT4 GPU** setup, providing:

- âš¡ **2x faster training** using both GPUs simultaneously
- ğŸ›¡ï¸ **Fixed protobuf warnings** that were appearing
- ğŸ”„ **Automatic GPU detection** and configuration
- ğŸ’¾ **Smart checkpoint management** (keeps 4 most recent)
- ğŸ“Š **Optimized batch sizes** for multi-GPU training

## ğŸ”§ Fixes Applied

### 1. Protobuf Warnings Fixed âœ…

**Problem:**
```
WARNING: All log messages before absl::InitializeLog()...
E0000 00:00:1763395889.203344 Unable to register cuDNN factory...
AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'
```

**Solution:**
```python
# Fixed by:
1. Setting environment variables BEFORE imports
2. Downgrading protobuf to 3.20.3
3. Setting PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION='python'
```

### 2. Multi-GPU Support Added âš¡

**Problem:**
- Only 1 GPU was being used
- Second GPU was idle (wasted resources)

**Solution:**
```python
# Automatic detection and DataParallel wrapping
if n_gpus > 1:
    model = nn.DataParallel(model, device_ids=[0, 1])
    # Now both GPUs are utilized!
```

## ğŸ“Š Performance Comparison

| Configuration | GPUs | Batch Size | Effective Batch | Training Time |
|---------------|------|------------|-----------------|---------------|
| **Old (Single GPU)** | 1 | 8 | 64 | ~9 hours |
| **New (Multi-GPU)** | 2 | 16 | 128 | ~4.5 hours âš¡ |

**Speedup: ~2x faster!**

## ğŸ¯ How It Works

### GPU Detection

```python
n_gpus = torch.cuda.device_count()  # Detects 2 GPUs on Kaggle

if n_gpus > 1:
    print(f"âš¡ Using {n_gpus} GPUs")
    model = nn.DataParallel(model)
```

### Automatic Batch Size Adjustment

```python
CONFIG = {
    'batch_size': 16 if use_multi_gpu else 8,  # 16 per GPU
    'gradient_accumulation_steps': 4 if use_multi_gpu else 8,
}

# Effective batch size:
# Single GPU: 8 Ã— 8 = 64
# Multi-GPU:  16 Ã— 4 Ã— 2 = 128
```

### DataParallel Behavior

```python
# Input batch: [32, 512] (batch_size=16 Ã— 2 GPUs)
# 
# GPU 0 processes: [16, 512]  (first half)
# GPU 1 processes: [16, 512]  (second half)
#
# Gradients are automatically synchronized
# Loss is averaged across GPUs
```

## ğŸš€ Quick Start

### 1. Enable 2xT4 GPUs on Kaggle

**Important:** You must select the correct accelerator!

```
Notebook Settings (right sidebar):
â”œâ”€â”€ Accelerator: GPU T4 x 2  âš¡ (NOT just "GPU T4")
â”œâ”€â”€ Internet: ON
â””â”€â”€ Persistence: ON (optional)
```

### 2. Upload and Run

1. Upload `text_generation_kaggle_multigpu.ipynb` to Kaggle
2. Click **Run All**
3. Watch both GPUs work in parallel!

### 3. Verify Multi-GPU

Look for this output:
```
GPU CONFIGURATION
============================================================
Number of GPUs available: 2

GPU 0:
  Name: Tesla T4
  Memory: 15.75 GB

GPU 1:
  Name: Tesla T4
  Memory: 15.75 GB

âš¡ MULTI-GPU MODE: Will use 2 GPUs with DataParallel
```

## ğŸ“ˆ Training Configuration

### Optimized for 2xT4 GPUs

```python
CONFIG = {
    # Per-GPU batch size (doubled for multi-GPU)
    'batch_size': 16,  # 16 per GPU = 32 total
    
    # Reduced accumulation (compensated by more GPUs)
    'gradient_accumulation_steps': 4,
    
    # Effective batch size: 16 Ã— 4 Ã— 2 = 128
    
    # Other settings remain the same
    'learning_rate': 5e-4,
    'epochs': 3,
    'max_checkpoints': 4,
}
```

### Memory Usage

```
Single GPU Mode:
â”œâ”€â”€ Model: ~40 MB
â”œâ”€â”€ Optimizer: ~40 MB
â”œâ”€â”€ Activations: ~10 GB
â””â”€â”€ Total: ~10-12 GB per GPU

Multi-GPU Mode:
â”œâ”€â”€ Model: ~40 MB (replicated on each GPU)
â”œâ”€â”€ Optimizer: ~40 MB (replicated on each GPU)
â”œâ”€â”€ Activations: ~5 GB per GPU (split)
â””â”€â”€ Total: ~6-8 GB per GPU
```

## ğŸ” Monitoring GPU Usage

### During Training

The progress bar shows:
```
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1234/1234 [15:30<00:00]
loss=3.2456 lr=5.00e-04 gpus=2
```

### Check GPU Utilization

In a code cell, run:
```python
!nvidia-smi
```

You should see both GPUs at ~90-100% utilization:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.xx.xx    Driver Version: 525.xx.xx    CUDA Version: 12.0   |
|-------------------------------+----------------------+----------------------+
|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |
| N/A   65C    P0    70W / 70W |  14000MiB / 15360MiB |     95%      Default |
|-------------------------------+----------------------+----------------------+
|   1  Tesla T4            On   | 00000000:00:05.0 Off |                    0 |
| N/A   64C    P0    69W / 70W |  14000MiB / 15360MiB |     94%      Default |
+-----------------------------------------------------------------------------+
```

## âš™ï¸ Advanced Configuration

### Adjust for Different GPU Counts

The notebook automatically adapts:

```python
# 1 GPU detected
batch_size = 8
gradient_accumulation = 8
effective_batch = 64

# 2 GPUs detected
batch_size = 16
gradient_accumulation = 4
effective_batch = 128

# 4 GPUs (if available)
batch_size = 32
gradient_accumulation = 2
effective_batch = 256
```

### Custom Batch Sizes

```python
# For more aggressive training (if memory allows)
CONFIG = {
    'batch_size': 24,  # 24 per GPU
    'gradient_accumulation_steps': 2,
    # Effective: 24 Ã— 2 Ã— 2 = 96
}

# For memory-constrained scenarios
CONFIG = {
    'batch_size': 12,  # 12 per GPU
    'gradient_accumulation_steps': 8,
    # Effective: 12 Ã— 8 Ã— 2 = 192
}
```

## ğŸ› Troubleshooting

### Problem: Only 1 GPU Detected

**Symptoms:**
```
Number of GPUs available: 1
âš ï¸ Single GPU mode (only 1 GPU detected)
```

**Solution:**
1. Check notebook settings
2. Ensure you selected **"GPU T4 x 2"** not just "GPU T4"
3. Restart the notebook
4. Re-run all cells

### Problem: Protobuf Warnings Still Appear

**Symptoms:**
```
AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'
```

**Solution:**
The notebook fixes this automatically, but if warnings persist:
```python
# Run this cell first:
!pip uninstall -y protobuf
!pip install -q protobuf==3.20.3
```

### Problem: Out of Memory with Multi-GPU

**Symptoms:**
```
RuntimeError: CUDA out of memory
```

**Solution:**
Reduce per-GPU batch size:
```python
CONFIG = {
    'batch_size': 12,  # Reduce from 16
    'gradient_accumulation_steps': 6,  # Increase to compensate
}
```

### Problem: Uneven GPU Utilization

**Symptoms:**
- GPU 0: 95% utilization
- GPU 1: 60% utilization

**Cause:** DataParallel has some overhead on GPU 0 (master GPU)

**Solution:** This is normal. GPU 0 handles:
- Gradient gathering
- Loss computation
- Optimizer updates

The slight imbalance is expected and doesn't significantly impact performance.

## ğŸ“Š Expected Training Times

### On 2xT4 GPUs

| Metric | Single GPU | Multi-GPU (2x) | Speedup |
|--------|------------|----------------|---------|
| Per Epoch | ~3 hours | ~1.5 hours | 2x |
| Full Training (3 epochs) | ~9 hours | ~4.5 hours | 2x |
| Per 1000 steps | ~20 min | ~10 min | 2x |

### Factors Affecting Speed

- **Data loading**: Use `num_workers=2` (already set)
- **Gradient accumulation**: Lower is faster (but larger effective batch)
- **Sequence length**: Shorter = faster
- **Model size**: Smaller = faster

## ğŸ’¡ Best Practices

### 1. Always Use Multi-GPU on Kaggle

If 2 GPUs are available, always use them:
- âœ… 2x faster training
- âœ… No code changes needed
- âœ… Automatic load balancing
- âœ… Same final model quality

### 2. Monitor Both GPUs

Check periodically:
```python
!nvidia-smi
```

Both should show:
- High utilization (>80%)
- Similar memory usage
- Similar temperature

### 3. Adjust Batch Size for Memory

If you see OOM errors:
```python
# Start conservative
batch_size = 12

# Gradually increase
batch_size = 14
batch_size = 16
batch_size = 18  # Until OOM, then back off
```

### 4. Keep Effective Batch Size Constant

When changing GPU count, maintain effective batch:
```python
# 1 GPU: 8 Ã— 8 = 64
# 2 GPU: 16 Ã— 4 = 64 (per GPU) Ã— 2 = 128

# To keep same effective batch:
# 2 GPU: 8 Ã— 4 = 32 (per GPU) Ã— 2 = 64
```

## ğŸ“ Technical Details

### DataParallel vs DistributedDataParallel

This notebook uses **DataParallel** because:
- âœ… Simpler to implement
- âœ… Works well for 2 GPUs
- âœ… No process management needed
- âœ… Automatic on Kaggle

For 4+ GPUs, consider **DistributedDataParallel** (DDP):
- Faster communication
- Better scaling
- More complex setup

### How DataParallel Works

```python
# 1. Model is replicated on each GPU
GPU 0: model_copy_0
GPU 1: model_copy_1

# 2. Batch is split across GPUs
Input [32, 512] â†’
  GPU 0: [16, 512]
  GPU 1: [16, 512]

# 3. Forward pass in parallel
GPU 0: loss_0 = model_copy_0(batch_0)
GPU 1: loss_1 = model_copy_1(batch_1)

# 4. Losses gathered and averaged on GPU 0
loss = (loss_0 + loss_1) / 2

# 5. Backward pass
loss.backward()  # Gradients computed on each GPU

# 6. Gradients gathered and averaged on GPU 0
# 7. Optimizer step on GPU 0
# 8. Updated weights broadcast to all GPUs
```

### Checkpoint Compatibility

Checkpoints saved from multi-GPU training:
- âœ… Can be loaded on single GPU
- âœ… Can be loaded on multi-GPU
- âœ… Model weights are identical
- âœ… No conversion needed

The notebook automatically handles DataParallel wrapping/unwrapping.

## ğŸ“ Summary

### What Changed

| Aspect | Old Notebook | New Multi-GPU Notebook |
|--------|--------------|------------------------|
| GPU Usage | 1 GPU (50% resources) | 2 GPUs (100% resources) |
| Training Speed | ~9 hours | ~4.5 hours |
| Batch Size | 8 per step | 32 per step |
| Protobuf Warnings | âŒ Present | âœ… Fixed |
| GPU Detection | Manual | Automatic |
| Configuration | Static | Dynamic |

### Key Benefits

1. âš¡ **2x faster training** - Use both GPUs
2. ğŸ›¡ï¸ **No warnings** - Clean output
3. ğŸ”„ **Automatic** - Detects and configures GPUs
4. ğŸ’¾ **Same checkpointing** - Keeps 4 most recent
5. ğŸ“Š **Better monitoring** - Shows GPU count in progress

### Ready to Use!

Upload `text_generation_kaggle_multigpu.ipynb` to Kaggle, enable 2xT4 GPUs, and enjoy 2x faster training! ğŸš€
