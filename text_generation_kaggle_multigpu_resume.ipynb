{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation Model - Multi-GPU with Accurate Resume\n",
    "## 10M Parameter GPT-2 with 2xT4 GPU + Perfect Resume Support\n",
    "\n",
    "**Features:**\n",
    "- ‚ö° Multi-GPU training (2xT4 on Kaggle)\n",
    "- üîÑ **ACCURATE RESUME** - Continue from exact step (e.g., step 20,000)\n",
    "- üíæ Automatic checkpoint management (keeps 4 most recent)\n",
    "- üõ°Ô∏è Fixed protobuf warnings\n",
    "- üìä Global step tracking across epochs\n",
    "- üéØ No training loss - picks up exactly where it left off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Fix Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix protobuf warnings FIRST\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Running on Kaggle: {'/kaggle/working' in sys.path or 'KAGGLE_KERNEL_RUN_TYPE' in os.environ}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix protobuf version\n",
    "!pip uninstall -y protobuf 2>/dev/null\n",
    "!pip install -q protobuf==3.20.3\n",
    "print(\"‚úì Protobuf fixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parallel import DataParallel\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import math\n",
    "\n",
    "print(\"‚úì All imports successful\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-GPU Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect GPUs\n",
    "if torch.cuda.is_available():\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GPU CONFIGURATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"GPUs available: {n_gpus}\")\n",
    "    \n",
    "    for i in range(n_gpus):\n",
    "        print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    device = torch.device('cuda:0')\n",
    "    use_multi_gpu = n_gpus > 1\n",
    "    \n",
    "    if use_multi_gpu:\n",
    "        print(f\"\\n‚ö° MULTI-GPU MODE: Using {n_gpus} GPUs\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU! Enable GPU in settings.\")\n",
    "    device = torch.device('cpu')\n",
    "    use_multi_gpu = False\n",
    "    n_gpus = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Model\n",
    "    'vocab_size': 50257,\n",
    "    'n_positions': 512,\n",
    "    'n_embd': 256,\n",
    "    'n_layer': 8,\n",
    "    'n_head': 8,\n",
    "    'n_inner': 1024,\n",
    "    \n",
    "    # Training (optimized for multi-GPU)\n",
    "    'batch_size': 16 if use_multi_gpu else 8,\n",
    "    'gradient_accumulation_steps': 4 if use_multi_gpu else 8,\n",
    "    'learning_rate': 5e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'epochs': 3,\n",
    "    'warmup_steps': 500,\n",
    "    'max_length': 512,\n",
    "    \n",
    "    # Checkpointing\n",
    "    'save_steps': 1000,\n",
    "    'eval_steps': 500,\n",
    "    'max_checkpoints': 4,\n",
    "    'checkpoint_dir': '/kaggle/working/checkpoints',\n",
    "    \n",
    "    # Dataset\n",
    "    'dataset_name': 'wikitext',\n",
    "    'dataset_config': 'wikitext-103-v1',\n",
    "    \n",
    "    # Multi-GPU\n",
    "    'use_multi_gpu': use_multi_gpu,\n",
    "    'n_gpus': n_gpus,\n",
    "    \n",
    "    # ‚≠ê RESUME SETTINGS\n",
    "    'resume_from_checkpoint': None,  # Set to checkpoint path to resume\n",
    "    # Example: '/kaggle/input/my-checkpoint/checkpoint_epoch2_step20000.pt'\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "effective_batch = CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']\n",
    "if use_multi_gpu:\n",
    "    effective_batch *= n_gpus\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"GPUs: {n_gpus}\")\n",
    "print(f\"Per-GPU Batch: {CONFIG['batch_size']}\")\n",
    "print(f\"Gradient Accumulation: {CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"Effective Batch: {effective_batch}\")\n",
    "print(f\"Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"Save Every: {CONFIG['save_steps']} steps\")\n",
    "print(f\"Keep: {CONFIG['max_checkpoints']} checkpoints\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Enhanced Checkpoint Management with Accurate Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_list(checkpoint_dir):\n",
    "    \"\"\"Get sorted list of checkpoints\"\"\"\n",
    "    checkpoints = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_*.pt'))\n",
    "    checkpoints.sort(key=os.path.getmtime, reverse=True)\n",
    "    return checkpoints\n",
    "\n",
    "def cleanup_old_checkpoints(checkpoint_dir, max_keep=4):\n",
    "    \"\"\"Keep only N most recent checkpoints\"\"\"\n",
    "    checkpoints = get_checkpoint_list(checkpoint_dir)\n",
    "    if len(checkpoints) > max_keep:\n",
    "        for ckpt in checkpoints[max_keep:]:\n",
    "            try:\n",
    "                os.remove(ckpt)\n",
    "                print(f\"  Deleted: {os.path.basename(ckpt)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: {e}\")\n",
    "\n",
    "def save_checkpoint(filepath, model, optimizer, scheduler, epoch, step, global_step, train_loss, val_loss=None, config=None):\n",
    "    \"\"\"Save checkpoint with ACCURATE resume information\"\"\"\n",
    "    try:\n",
    "        # Unwrap DataParallel\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model_state = model.module.state_dict()\n",
    "        else:\n",
    "            model_state = model.state_dict()\n",
    "        \n",
    "        checkpoint = {\n",
    "            # ‚≠ê CRITICAL: Track both epoch step and global step\n",
    "            'epoch': epoch,\n",
    "            'step': step,  # Step within current epoch\n",
    "            'global_step': global_step,  # Total steps across ALL epochs\n",
    "            \n",
    "            # Model and optimizer states\n",
    "            'model_state_dict': model_state,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            \n",
    "            # Losses\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            \n",
    "            # Metadata\n",
    "            'config': config,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'pytorch_version': torch.__version__,\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, filepath)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer=None, scheduler=None):\n",
    "    \"\"\"Load checkpoint with ACCURATE resume\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nLoading checkpoint: {filepath}\")\n",
    "        checkpoint = torch.load(filepath, map_location='cpu')\n",
    "        \n",
    "        # Load model (handle DataParallel)\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Load optimizer\n",
    "        if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(\"‚úì Optimizer state loaded\")\n",
    "        \n",
    "        # Load scheduler\n",
    "        if scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            print(\"‚úì Scheduler state loaded\")\n",
    "        \n",
    "        # Return resume information\n",
    "        metadata = {\n",
    "            'epoch': checkpoint.get('epoch', 0),\n",
    "            'step': checkpoint.get('step', 0),\n",
    "            'global_step': checkpoint.get('global_step', 0),  # ‚≠ê CRITICAL\n",
    "            'train_loss': checkpoint.get('train_loss', None),\n",
    "            'val_loss': checkpoint.get('val_loss', None),\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úì Checkpoint loaded successfully\")\n",
    "        print(f\"  Epoch: {metadata['epoch']}\")\n",
    "        print(f\"  Step in epoch: {metadata['step']}\")\n",
    "        print(f\"  Global step: {metadata['global_step']}\")\n",
    "        print(f\"  Train loss: {metadata['train_loss']:.4f}\" if metadata['train_loss'] else \"\")\n",
    "        print(f\"  Val loss: {metadata['val_loss']:.4f}\" if metadata['val_loss'] else \"\")\n",
    "        \n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"‚úì Enhanced checkpoint functions defined\")\n",
    "print(\"  - Tracks global_step for accurate resume\")\n",
    "print(\"  - Can resume from exact step (e.g., 20,000)\")\n",
    "print(\"  - No training loss when resuming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model_config = GPT2Config(\n",
    "    vocab_size=CONFIG['vocab_size'],\n",
    "    n_positions=CONFIG['n_positions'],\n",
    "    n_embd=CONFIG['n_embd'],\n",
    "    n_layer=CONFIG['n_layer'],\n",
    "    n_head=CONFIG['n_head'],\n",
    "    n_inner=CONFIG['n_inner'],\n",
    "    activation_function='gelu_new',\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    attn_pdrop=0.1,\n",
    ")\n",
    "\n",
    "print(\"\\nInitializing model...\")\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "if use_multi_gpu:\n",
    "    print(f\"‚ö° Wrapping with DataParallel for {n_gpus} GPUs\")\n",
    "    model = nn.DataParallel(model, device_ids=list(range(n_gpus)))\n",
    "\n",
    "total_params = sum(p.numel() for p in (model.module if isinstance(model, nn.DataParallel) else model).parameters())\n",
    "print(f\"‚úì Model ready: {total_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Dataset\n",
    "print(\"\\nLoading dataset...\")\n",
    "dataset = load_dataset(CONFIG['dataset_name'], CONFIG['dataset_config'])\n",
    "print(f\"‚úì Train: {len(dataset['train']):,} samples\")\n",
    "print(f\"‚úì Val: {len(dataset['validation']):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=CONFIG['max_length'],\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "tokenized_train = dataset['train'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "tokenized_val = dataset['validation'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['validation'].column_names\n",
    ")\n",
    "tokenized_train.set_format('torch')\n",
    "tokenized_val.set_format('torch')\n",
    "print(\"‚úì Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    tokenized_train,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    tokenized_val,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "print(f\"‚úì DataLoaders: {len(train_loader):,} train batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "total_steps = (len(train_loader) * CONFIG['epochs']) // CONFIG['gradient_accumulation_steps']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=CONFIG['warmup_steps'],\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Optimizer and scheduler ready\")\n",
    "print(f\"  Total training steps: {total_steps:,}\")\n",
    "print(f\"  Warmup steps: {CONFIG['warmup_steps']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resume from Checkpoint (If Specified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training state\n",
    "start_epoch = 1\n",
    "start_step = 0\n",
    "global_step = 0  # ‚≠ê CRITICAL: Tracks total steps across all epochs\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Resume if checkpoint specified\n",
    "if CONFIG['resume_from_checkpoint'] and os.path.exists(CONFIG['resume_from_checkpoint']):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESUMING FROM CHECKPOINT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    metadata = load_checkpoint(\n",
    "        CONFIG['resume_from_checkpoint'],\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler\n",
    "    )\n",
    "    \n",
    "    if metadata:\n",
    "        start_epoch = metadata['epoch']\n",
    "        start_step = metadata['step']\n",
    "        global_step = metadata['global_step']  # ‚≠ê Resume from exact global step\n",
    "        \n",
    "        if metadata['val_loss']:\n",
    "            best_val_loss = metadata['val_loss']\n",
    "        \n",
    "        print(f\"\\n‚ö° WILL RESUME FROM:\")\n",
    "        print(f\"  Epoch: {start_epoch}\")\n",
    "        print(f\"  Step in epoch: {start_step}\")\n",
    "        print(f\"  Global step: {global_step}\")\n",
    "        print(f\"  (Will skip first {start_step} steps of epoch {start_epoch})\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Failed to load checkpoint, starting from scratch\")\n",
    "else:\n",
    "    print(\"\\n‚úì Starting training from scratch (no checkpoint specified)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Functions with Accurate Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, device, epoch, config, start_step=0, global_step=0):\n",
    "    \"\"\"Train one epoch with ACCURATE resume from start_step\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # ‚≠ê SKIP steps if resuming\n",
    "        if step < start_step:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss / config['gradient_accumulation_steps']\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1  # ‚≠ê Increment global step\n",
    "            \n",
    "            total_loss += loss.item() * config['gradient_accumulation_steps']\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{loss.item() * config['gradient_accumulation_steps']:.4f}\",\n",
    "                'global_step': global_step,  # ‚≠ê Show global step\n",
    "                'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "            })\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (step + 1) % config['save_steps'] == 0:\n",
    "                checkpoint_path = os.path.join(\n",
    "                    config['checkpoint_dir'],\n",
    "                    f\"checkpoint_epoch{epoch}_step{step+1}_global{global_step}.pt\"\n",
    "                )\n",
    "                \n",
    "                if save_checkpoint(\n",
    "                    checkpoint_path,\n",
    "                    model,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    epoch,\n",
    "                    step + 1,\n",
    "                    global_step,  # ‚≠ê Save global step\n",
    "                    total_loss / (step + 1 - start_step),\n",
    "                    config=config\n",
    "                ):\n",
    "                    print(f\"\\n‚úì Saved: {os.path.basename(checkpoint_path)}\")\n",
    "                    print(f\"  Global step: {global_step}\")\n",
    "                    cleanup_old_checkpoints(config['checkpoint_dir'], config['max_checkpoints'])\n",
    "        \n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e):\n",
    "                print(f\"\\n‚ö†Ô∏è OOM at step {step}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    return total_loss / (len(loader) - start_step), global_step\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=input_ids\n",
    "                )\n",
    "                total_loss += outputs.loss.item()\n",
    "            except RuntimeError as e:\n",
    "                if 'out of memory' in str(e):\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    perplexity = math.exp(avg_loss) if avg_loss < 100 else float('inf')\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "print(\"‚úì Training functions ready with accurate resume support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Main Training Loop with Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "training_history = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "if start_step > 0:\n",
    "    print(f\"‚ö° RESUMING from epoch {start_epoch}, step {start_step}\")\n",
    "    print(f\"‚ö° Global step: {global_step}\")\n",
    "else:\n",
    "    print(f\"Starting fresh from epoch 1\")\n",
    "if use_multi_gpu:\n",
    "    print(f\"‚ö° Using {n_gpus} GPUs\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, CONFIG['epochs'] + 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EPOCH {epoch}/{CONFIG['epochs']}\")\n",
    "        if epoch == start_epoch and start_step > 0:\n",
    "            print(f\"(Resuming from step {start_step})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Determine if we need to skip steps (only for first resumed epoch)\n",
    "        skip_steps = start_step if epoch == start_epoch else 0\n",
    "        \n",
    "        # Train\n",
    "        train_loss, global_step = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            device,\n",
    "            epoch,\n",
    "            CONFIG,\n",
    "            start_step=skip_steps,\n",
    "            global_step=global_step\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_perplexity = evaluate(model, val_loader, device)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch} Results:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Perplexity: {val_perplexity:.2f}\")\n",
    "        print(f\"  Global Step: {global_step}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Save history\n",
    "        training_history.append({\n",
    "            'epoch': epoch,\n",
    "            'global_step': global_step,\n",
    "            'train_loss': float(train_loss),\n",
    "            'val_loss': float(val_loss),\n",
    "            'val_perplexity': float(val_perplexity),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_path = '/kaggle/working/best_model.pt'\n",
    "            \n",
    "            if save_checkpoint(\n",
    "                best_path,\n",
    "                model,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                epoch,\n",
    "                len(train_loader),\n",
    "                global_step,\n",
    "                train_loss,\n",
    "                val_loss,\n",
    "                CONFIG\n",
    "            ):\n",
    "                print(f\"\\n‚úì New best model! (val_loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Save epoch checkpoint\n",
    "        epoch_path = os.path.join(\n",
    "            CONFIG['checkpoint_dir'],\n",
    "            f\"checkpoint_epoch{epoch}_final_global{global_step}.pt\"\n",
    "        )\n",
    "        \n",
    "        save_checkpoint(\n",
    "            epoch_path,\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            epoch,\n",
    "            len(train_loader),\n",
    "            global_step,\n",
    "            train_loss,\n",
    "            val_loss,\n",
    "            CONFIG\n",
    "        )\n",
    "        print(f\"‚úì Epoch {epoch} checkpoint saved\")\n",
    "        \n",
    "        # Cleanup\n",
    "        cleanup_old_checkpoints(CONFIG['checkpoint_dir'], CONFIG['max_checkpoints'])\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Reset start_step after first epoch\n",
    "        start_step = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úì TRAINING COMPLETED!\")\n",
    "    print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
    "    print(f\"  Total global steps: {global_step}\")\n",
    "    if use_multi_gpu:\n",
    "        print(f\"  Trained on {n_gpus} GPUs\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Interrupted! Saving emergency checkpoint...\")\n",
    "    emergency_path = '/kaggle/working/emergency_checkpoint.pt'\n",
    "    save_checkpoint(\n",
    "        emergency_path,\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        epoch,\n",
    "        step,\n",
    "        global_step,\n",
    "        train_loss,\n",
    "        config=CONFIG\n",
    "    )\n",
    "    print(f\"‚úì Emergency checkpoint saved\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history\n",
    "history_path = '/kaggle/working/training_history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for entry in training_history:\n",
    "    print(f\"Epoch {entry['epoch']} (step {entry['global_step']}): \"\n",
    "          f\"Train={entry['train_loss']:.4f}, \"\n",
    "          f\"Val={entry['val_loss']:.4f}, \"\n",
    "          f\"PPL={entry['val_perplexity']:.2f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚úì History saved to {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Text Generation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100, temperature=0.8):\n",
    "    \"\"\"Generate text\"\"\"\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        gen_model = model.module\n",
    "    else:\n",
    "        gen_model = model\n",
    "    \n",
    "    gen_model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = gen_model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test\n",
    "test_prompts = [\n",
    "    \"The future of artificial intelligence\",\n",
    "    \"In a world where technology\",\n",
    "    \"Scientists have discovered\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEXT GENERATION EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"‚îÄ\" * 60)\n",
    "    try:\n",
    "        generated = generate_text(prompt, max_length=150)\n",
    "        print(generated)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in HuggingFace format\n",
    "final_dir = '/kaggle/working/final_model'\n",
    "print(f\"\\nSaving final model...\")\n",
    "\n",
    "try:\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        save_model = model.module\n",
    "    else:\n",
    "        save_model = model\n",
    "    \n",
    "    save_model.save_pretrained(final_dir)\n",
    "    tokenizer.save_pretrained(final_dir)\n",
    "    \n",
    "    with open(os.path.join(final_dir, 'training_config.json'), 'w') as f:\n",
    "        json.dump(CONFIG, f, indent=2)\n",
    "    \n",
    "    print(\"‚úì Model saved\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Output Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUTPUT FILES\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìÅ /kaggle/working/\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ best_model.pt (best checkpoint)\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ training_history.json\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ checkpoints/\")\n",
    "\n",
    "checkpoints = get_checkpoint_list(CONFIG['checkpoint_dir'])\n",
    "if checkpoints:\n",
    "    for ckpt in checkpoints:\n",
    "        size = os.path.getsize(ckpt) / 1e6\n",
    "        print(f\"  ‚îÇ   ‚îú‚îÄ‚îÄ {os.path.basename(ckpt)} ({size:.1f} MB)\")\n",
    "\n",
    "print(\"  ‚îî‚îÄ‚îÄ final_model/ (HuggingFace format)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if use_multi_gpu:\n",
    "    print(f\"\\n‚ö° Trained on {n_gpus} GPUs\")\n",
    "print(f\"\\n‚úì Total global steps: {global_step}\")\n",
    "print(\"\\n‚úì Training complete! Download from Output tab.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TO RESUME TRAINING:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Upload any checkpoint to Kaggle Datasets\")\n",
    "print(\"2. Add dataset to notebook\")\n",
    "print(\"3. Set CONFIG['resume_from_checkpoint'] = '/kaggle/input/...'\")\n",
    "print(\"4. Run notebook - it will continue from exact step!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "isGpuEnabled": true,
   "isInternetEnabled": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
