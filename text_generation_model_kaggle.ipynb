{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20M Parameter Text Generation Model - Kaggle Training\n",
    "## Resume Training from JSON Checkpoint\n",
    "\n",
    "This notebook trains a transformer model on Kaggle, with support for resuming from JSON checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q transformers datasets tokenizers accelerate\n",
    "!pip install -q sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'batch_size': 8,  # Reduced for Kaggle\n",
    "    'learning_rate': 5e-4,\n",
    "    'epochs': 3,\n",
    "    'warmup_steps': 500,\n",
    "    'gradient_accumulation_steps': 8,  # Increased to compensate for smaller batch\n",
    "    'max_grad_norm': 1.0,\n",
    "    'save_steps': 500,\n",
    "    'eval_steps': 500,\n",
    "    'max_length': 512,\n",
    "    'resume_from_json': False,  # Set to True if resuming\n",
    "    'json_checkpoint_path': '/kaggle/input/your-checkpoint/checkpoint.json'  # Update this\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture (10M parameters)\n",
    "model_config = GPT2Config(\n",
    "    vocab_size=50257,\n",
    "    n_positions=512,\n",
    "    n_embd=256,\n",
    "    n_layer=8,\n",
    "    n_head=8,\n",
    "    n_inner=1024,\n",
    "    activation_function='gelu_new',\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    attn_pdrop=0.1,\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1e6:.2f} MB (FP32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. JSON Checkpoint Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_from_json(json_path, model, optimizer=None):\n",
    "    \"\"\"\n",
    "    Load checkpoint from JSON format\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to JSON checkpoint\n",
    "        model: Model to load weights into\n",
    "        optimizer: Optimizer to load state into (optional)\n",
    "    \n",
    "    Returns:\n",
    "        metadata: Dictionary with epoch, step, loss info\n",
    "    \"\"\"\n",
    "    print(f\"Loading checkpoint from: {json_path}\")\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "    \n",
    "    # Load model state\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        print(\"Loading model weights...\")\n",
    "        model_state = {}\n",
    "        for key, value in checkpoint['model_state_dict'].items():\n",
    "            if isinstance(value, dict) and 'data' in value:\n",
    "                model_state[key] = torch.tensor(value['data'])\n",
    "            else:\n",
    "                model_state[key] = value\n",
    "        \n",
    "        model.load_state_dict(model_state)\n",
    "        print(\"✓ Model weights loaded\")\n",
    "    \n",
    "    # Load optimizer state\n",
    "    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "        print(\"Loading optimizer state...\")\n",
    "        opt_state = checkpoint['optimizer_state_dict']\n",
    "        \n",
    "        # Reconstruct optimizer state\n",
    "        optimizer_state = {\n",
    "            'state': {},\n",
    "            'param_groups': opt_state.get('param_groups', [])\n",
    "        }\n",
    "        \n",
    "        for param_id, param_state in opt_state.get('state', {}).items():\n",
    "            optimizer_state['state'][int(param_id)] = {}\n",
    "            for key, value in param_state.items():\n",
    "                if isinstance(value, dict) and 'data' in value:\n",
    "                    optimizer_state['state'][int(param_id)][key] = torch.tensor(value['data'])\n",
    "                else:\n",
    "                    optimizer_state['state'][int(param_id)][key] = value\n",
    "        \n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "        print(\"✓ Optimizer state loaded\")\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata = {\n",
    "        'epoch': checkpoint.get('epoch', 0),\n",
    "        'step': checkpoint.get('step', 0),\n",
    "        'val_loss': checkpoint.get('val_loss', None),\n",
    "        'train_loss': checkpoint.get('train_loss', None)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nCheckpoint metadata:\")\n",
    "    for key, value in metadata.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def save_checkpoint_to_json(filepath, model, optimizer, epoch, step, train_loss, val_loss=None):\n",
    "    \"\"\"\n",
    "    Save checkpoint in JSON format\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to save JSON file\n",
    "        model: Model to save\n",
    "        optimizer: Optimizer to save\n",
    "        epoch: Current epoch\n",
    "        step: Current step\n",
    "        train_loss: Training loss\n",
    "        val_loss: Validation loss (optional)\n",
    "    \"\"\"\n",
    "    print(f\"Saving checkpoint to: {filepath}\")\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'step': step,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'model_state_dict': {},\n",
    "        'optimizer_state_dict': {\n",
    "            'state': {},\n",
    "            'param_groups': optimizer.state_dict()['param_groups']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Convert model state\n",
    "    for key, tensor in model.state_dict().items():\n",
    "        checkpoint['model_state_dict'][key] = {\n",
    "            'data': tensor.cpu().numpy().tolist(),\n",
    "            'shape': list(tensor.shape),\n",
    "            'dtype': str(tensor.dtype)\n",
    "        }\n",
    "    \n",
    "    # Convert optimizer state (simplified - only save essential parts)\n",
    "    opt_state = optimizer.state_dict()['state']\n",
    "    for param_id, param_state in opt_state.items():\n",
    "        checkpoint['optimizer_state_dict']['state'][str(param_id)] = {}\n",
    "        for key, value in param_state.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                checkpoint['optimizer_state_dict']['state'][str(param_id)][key] = {\n",
    "                    'data': value.cpu().numpy().tolist(),\n",
    "                    'shape': list(value.shape),\n",
    "                    'dtype': str(value.dtype)\n",
    "                }\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(checkpoint, f)\n",
    "    \n",
    "    print(\"✓ Checkpoint saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset('wikitext', 'wikitext-103-v1')\n",
    "\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=CONFIG['max_length'],\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = dataset['train'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "tokenized_val = dataset['validation'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['validation'].column_names\n",
    ")\n",
    "\n",
    "tokenized_train.set_format('torch')\n",
    "tokenized_val.set_format('torch')\n",
    "\n",
    "print(\"✓ Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    tokenized_train,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_val,\n",
    "    batch_size=CONFIG['batch_size']\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "total_steps = len(train_loader) * CONFIG['epochs'] // CONFIG['gradient_accumulation_steps']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=CONFIG['warmup_steps'],\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "\n",
    "# Resume from checkpoint if specified\n",
    "start_epoch = 1\n",
    "global_step = 0\n",
    "\n",
    "if CONFIG['resume_from_json'] and os.path.exists(CONFIG['json_checkpoint_path']):\n",
    "    metadata = load_checkpoint_from_json(\n",
    "        CONFIG['json_checkpoint_path'],\n",
    "        model,\n",
    "        optimizer\n",
    "    )\n",
    "    start_epoch = metadata['epoch'] + 1\n",
    "    global_step = metadata['step']\n",
    "    print(f\"\\n✓ Resuming from epoch {start_epoch}, step {global_step}\")\n",
    "else:\n",
    "    print(\"\\nStarting training from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, device, epoch, start_step=0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        if step < start_step:\n",
    "            continue\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=input_ids\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss / CONFIG['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % CONFIG['gradient_accumulation_steps'] == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * CONFIG['gradient_accumulation_steps']\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item() * CONFIG['gradient_accumulation_steps'],\n",
    "            'lr': scheduler.get_last_lr()[0]\n",
    "        })\n",
    "        \n",
    "        # Save checkpoint periodically\n",
    "        if (step + 1) % CONFIG['save_steps'] == 0:\n",
    "            checkpoint_path = f'/kaggle/working/checkpoint_epoch{epoch}_step{step+1}.json'\n",
    "            save_checkpoint_to_json(\n",
    "                checkpoint_path,\n",
    "                model,\n",
    "                optimizer,\n",
    "                epoch,\n",
    "                step + 1,\n",
    "                total_loss / (step + 1)\n",
    "            )\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            total_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return avg_loss, perplexity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "training_history = []\n",
    "\n",
    "for epoch in range(start_epoch, CONFIG['epochs'] + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch}/{CONFIG['epochs']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        epoch\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_perplexity = evaluate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val Perplexity: {val_perplexity:.2f}\")\n",
    "    \n",
    "    training_history.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'val_perplexity': val_perplexity\n",
    "    })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_checkpoint_to_json(\n",
    "            '/kaggle/working/best_model.json',\n",
    "            model,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            len(train_loader),\n",
    "            train_loss,\n",
    "            val_loss\n",
    "        )\n",
    "        print(\"✓ Saved best model\")\n",
    "    \n",
    "    # Save epoch checkpoint\n",
    "    save_checkpoint_to_json(\n",
    "        f'/kaggle/working/checkpoint_epoch{epoch}.json',\n",
    "        model,\n",
    "        optimizer,\n",
    "        epoch,\n",
    "        len(train_loader),\n",
    "        train_loss,\n",
    "        val_loss\n",
    "    )\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "with open('/kaggle/working/training_history.json', 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "print(\"Training history saved!\")\n",
    "print(\"\\nFinal Results:\")\n",
    "for entry in training_history:\n",
    "    print(f\"Epoch {entry['epoch']}: Train Loss={entry['train_loss']:.4f}, \"\n",
    "          f\"Val Loss={entry['val_loss']:.4f}, Perplexity={entry['val_perplexity']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Text Generation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100, temperature=0.8):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test generation\n",
    "test_prompts = [\n",
    "    \"The future of artificial intelligence\",\n",
    "    \"In a world where technology\",\n",
    "    \"Scientists have discovered\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Text Generation Examples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 60)\n",
    "    generated = generate_text(prompt, max_length=150)\n",
    "    print(generated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in HuggingFace format\n",
    "model.save_pretrained('/kaggle/working/final_model')\n",
    "tokenizer.save_pretrained('/kaggle/working/final_model')\n",
    "\n",
    "print(\"✓ Model saved in HuggingFace format\")\n",
    "print(\"\\nOutput files:\")\n",
    "print(\"  - best_model.json (best checkpoint)\")\n",
    "print(\"  - checkpoint_epoch*.json (epoch checkpoints)\")\n",
    "print(\"  - training_history.json (training metrics)\")\n",
    "print(\"  - final_model/ (HuggingFace format)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
