{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation Model - Production Kaggle Training\n",
    "## 10M Parameter GPT-2 Style Model with Smart Checkpointing\n",
    "\n",
    "**Features:**\n",
    "- Train from scratch with proper error handling\n",
    "- Automatic checkpoint management (keeps 4 most recent)\n",
    "- Kaggle-optimized memory usage\n",
    "- Resume training capability\n",
    "- Progress tracking and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Running on Kaggle: {'/kaggle/working' in sys.path or 'KAGGLE_KERNEL_RUN_TYPE' in __import__('os').environ}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle has most packages pre-installed, but let's ensure versions are compatible\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Only install if needed\n",
    "try:\n",
    "    import transformers\n",
    "    import datasets\n",
    "    print(\"‚úì Core packages already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing missing packages...\")\n",
    "    install_package('transformers>=4.30.0')\n",
    "    install_package('datasets>=2.12.0')\n",
    "    print(\"‚úì Packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import math\n",
    "\n",
    "print(\"‚úì All imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Datasets version: {datasets.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU Check and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "    \n",
    "    # Enable TF32 for better performance on Ampere GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(\"‚úì TF32 enabled for faster training\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU detected! Training will be extremely slow.\")\n",
    "    print(\"Please enable GPU in Kaggle notebook settings.\")\n",
    "\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Model architecture\n",
    "    'vocab_size': 50257,\n",
    "    'n_positions': 512,\n",
    "    'n_embd': 256,\n",
    "    'n_layer': 8,\n",
    "    'n_head': 8,\n",
    "    'n_inner': 1024,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'batch_size': 8,\n",
    "    'gradient_accumulation_steps': 8,  # Effective batch size = 64\n",
    "    'learning_rate': 5e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'epochs': 3,\n",
    "    'warmup_steps': 500,\n",
    "    'max_length': 512,\n",
    "    \n",
    "    # Checkpointing\n",
    "    'save_steps': 1000,\n",
    "    'eval_steps': 500,\n",
    "    'max_checkpoints': 4,  # Keep only 4 most recent checkpoints\n",
    "    'checkpoint_dir': '/kaggle/working/checkpoints',\n",
    "    \n",
    "    # Dataset\n",
    "    'dataset_name': 'wikitext',\n",
    "    'dataset_config': 'wikitext-103-v1',\n",
    "    \n",
    "    # Resume training\n",
    "    'resume_from_checkpoint': None,  # Set to checkpoint path to resume\n",
    "}\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model Parameters: ~10M\")\n",
    "print(f\"Batch Size: {CONFIG['batch_size']}\")\n",
    "print(f\"Gradient Accumulation: {CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"Effective Batch Size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"Learning Rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"Max Checkpoints: {CONFIG['max_checkpoints']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Checkpoint Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_list(checkpoint_dir):\n",
    "    \"\"\"Get sorted list of checkpoint files\"\"\"\n",
    "    checkpoints = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_*.pt'))\n",
    "    # Sort by modification time (newest first)\n",
    "    checkpoints.sort(key=os.path.getmtime, reverse=True)\n",
    "    return checkpoints\n",
    "\n",
    "def cleanup_old_checkpoints(checkpoint_dir, max_keep=4):\n",
    "    \"\"\"Keep only the most recent N checkpoints\"\"\"\n",
    "    checkpoints = get_checkpoint_list(checkpoint_dir)\n",
    "    \n",
    "    if len(checkpoints) > max_keep:\n",
    "        to_delete = checkpoints[max_keep:]\n",
    "        for ckpt in to_delete:\n",
    "            try:\n",
    "                os.remove(ckpt)\n",
    "                print(f\"  Deleted old checkpoint: {os.path.basename(ckpt)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not delete {ckpt}: {e}\")\n",
    "\n",
    "def save_checkpoint(filepath, model, optimizer, scheduler, epoch, step, train_loss, val_loss=None, config=None):\n",
    "    \"\"\"Save training checkpoint\"\"\"\n",
    "    try:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'step': step,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'config': config,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, filepath)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving checkpoint: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer=None, scheduler=None):\n",
    "    \"\"\"Load training checkpoint\"\"\"\n",
    "    try:\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        return {\n",
    "            'epoch': checkpoint.get('epoch', 0),\n",
    "            'step': checkpoint.get('step', 0),\n",
    "            'train_loss': checkpoint.get('train_loss', None),\n",
    "            'val_loss': checkpoint.get('val_loss', None),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úì Checkpoint management functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model configuration\n",
    "model_config = GPT2Config(\n",
    "    vocab_size=CONFIG['vocab_size'],\n",
    "    n_positions=CONFIG['n_positions'],\n",
    "    n_embd=CONFIG['n_embd'],\n",
    "    n_layer=CONFIG['n_layer'],\n",
    "    n_head=CONFIG['n_head'],\n",
    "    n_inner=CONFIG['n_inner'],\n",
    "    activation_function='gelu_new',\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    attn_pdrop=0.1,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Vocabulary Size: {model_config.vocab_size:,}\")\n",
    "print(f\"Max Sequence Length: {model_config.n_positions}\")\n",
    "print(f\"Embedding Dimension: {model_config.n_embd}\")\n",
    "print(f\"Number of Layers: {model_config.n_layer}\")\n",
    "print(f\"Number of Attention Heads: {model_config.n_head}\")\n",
    "print(f\"FFN Inner Dimension: {model_config.n_inner}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(\"\\nInitializing model...\")\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n‚úì Model initialized successfully\")\n",
    "print(f\"  Total Parameters: {total_params:,}\")\n",
    "print(f\"  Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"  Model Size (FP32): {total_params * 4 / 1e6:.2f} MB\")\n",
    "print(f\"  Model Size (FP16): {total_params * 2 / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"\\nLoading tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"‚úì Tokenizer loaded (vocab size: {len(tokenizer):,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"\\nLoading dataset...\")\n",
    "try:\n",
    "    dataset = load_dataset(CONFIG['dataset_name'], CONFIG['dataset_config'])\n",
    "    print(f\"‚úì Dataset loaded successfully\")\n",
    "    print(f\"  Train samples: {len(dataset['train']):,}\")\n",
    "    print(f\"  Validation samples: {len(dataset['validation']):,}\")\n",
    "    print(f\"  Test samples: {len(dataset['test']):,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text examples\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=CONFIG['max_length'],\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "print(\"\\nTokenizing datasets...\")\n",
    "print(\"  This may take a few minutes...\")\n",
    "\n",
    "try:\n",
    "    tokenized_train = dataset['train'].map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset['train'].column_names,\n",
    "        desc=\"Tokenizing train set\"\n",
    "    )\n",
    "    \n",
    "    tokenized_val = dataset['validation'].map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset['validation'].column_names,\n",
    "        desc=\"Tokenizing validation set\"\n",
    "    )\n",
    "    \n",
    "    tokenized_train.set_format('torch')\n",
    "    tokenized_val.set_format('torch')\n",
    "    \n",
    "    print(\"‚úì Tokenization complete\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during tokenization: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "print(\"\\nCreating dataloaders...\")\n",
    "try:\n",
    "    train_loader = DataLoader(\n",
    "        tokenized_train,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        tokenized_val,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì DataLoaders created\")\n",
    "    print(f\"  Train batches: {len(train_loader):,}\")\n",
    "    print(f\"  Validation batches: {len(val_loader):,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating dataloaders: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimizer and Scheduler Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Calculate total training steps\n",
    "total_steps = (len(train_loader) * CONFIG['epochs']) // CONFIG['gradient_accumulation_steps']\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=CONFIG['warmup_steps'],\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SETUP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Training Steps: {total_steps:,}\")\n",
    "print(f\"Warmup Steps: {CONFIG['warmup_steps']:,}\")\n",
    "print(f\"Initial Learning Rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Weight Decay: {CONFIG['weight_decay']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resume from Checkpoint (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training from checkpoint if specified\n",
    "start_epoch = 1\n",
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "if CONFIG['resume_from_checkpoint'] and os.path.exists(CONFIG['resume_from_checkpoint']):\n",
    "    print(f\"\\nResuming from checkpoint: {CONFIG['resume_from_checkpoint']}\")\n",
    "    metadata = load_checkpoint(\n",
    "        CONFIG['resume_from_checkpoint'],\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler\n",
    "    )\n",
    "    \n",
    "    if metadata:\n",
    "        start_epoch = metadata['epoch'] + 1\n",
    "        global_step = metadata['step']\n",
    "        if metadata['val_loss']:\n",
    "            best_val_loss = metadata['val_loss']\n",
    "        print(f\"‚úì Resumed from epoch {metadata['epoch']}, step {global_step}\")\n",
    "        print(f\"  Previous train loss: {metadata['train_loss']:.4f}\")\n",
    "        if metadata['val_loss']:\n",
    "            print(f\"  Previous val loss: {metadata['val_loss']:.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚úì Starting training from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, device, epoch, config):\n",
    "    \"\"\"Train for one epoch with gradient accumulation\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        try:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            \n",
    "            # Scale loss for gradient accumulation\n",
    "            loss = outputs.loss / config['gradient_accumulation_steps']\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights after accumulation\n",
    "            if (step + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Track loss\n",
    "            total_loss += loss.item() * config['gradient_accumulation_steps']\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{loss.item() * config['gradient_accumulation_steps']:.4f}\",\n",
    "                'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "            })\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if (step + 1) % config['save_steps'] == 0:\n",
    "                checkpoint_path = os.path.join(\n",
    "                    config['checkpoint_dir'],\n",
    "                    f\"checkpoint_epoch{epoch}_step{step+1}.pt\"\n",
    "                )\n",
    "                \n",
    "                if save_checkpoint(\n",
    "                    checkpoint_path,\n",
    "                    model,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    epoch,\n",
    "                    step + 1,\n",
    "                    total_loss / (step + 1),\n",
    "                    config=config\n",
    "                ):\n",
    "                    print(f\"\\n‚úì Checkpoint saved: {os.path.basename(checkpoint_path)}\")\n",
    "                    cleanup_old_checkpoints(config['checkpoint_dir'], config['max_checkpoints'])\n",
    "        \n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e):\n",
    "                print(f\"\\n‚ö†Ô∏è OOM Error at step {step}. Clearing cache...\")\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate the model on validation set\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=input_ids\n",
    "                )\n",
    "                total_loss += outputs.loss.item()\n",
    "            \n",
    "            except RuntimeError as e:\n",
    "                if 'out of memory' in str(e):\n",
    "                    print(\"\\n‚ö†Ô∏è OOM during evaluation. Clearing cache...\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    perplexity = math.exp(avg_loss) if avg_loss < 100 else float('inf')\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "print(\"‚úì Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "training_history = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training from epoch {start_epoch} to {CONFIG['epochs']}\")\n",
    "print(f\"Checkpoints will be saved to: {CONFIG['checkpoint_dir']}\")\n",
    "print(f\"Keeping {CONFIG['max_checkpoints']} most recent checkpoints\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, CONFIG['epochs'] + 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EPOCH {epoch}/{CONFIG['epochs']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            device,\n",
    "            epoch,\n",
    "            CONFIG\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_perplexity = evaluate(model, val_loader, device)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch} Results:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Perplexity: {val_perplexity:.2f}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Save history\n",
    "        training_history.append({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': float(train_loss),\n",
    "            'val_loss': float(val_loss),\n",
    "            'val_perplexity': float(val_perplexity),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = '/kaggle/working/best_model.pt'\n",
    "            \n",
    "            if save_checkpoint(\n",
    "                best_model_path,\n",
    "                model,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                epoch,\n",
    "                len(train_loader),\n",
    "                train_loss,\n",
    "                val_loss,\n",
    "                CONFIG\n",
    "            ):\n",
    "                print(f\"\\n‚úì New best model saved! (val_loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Save epoch checkpoint\n",
    "        epoch_checkpoint_path = os.path.join(\n",
    "            CONFIG['checkpoint_dir'],\n",
    "            f\"checkpoint_epoch{epoch}_final.pt\"\n",
    "        )\n",
    "        \n",
    "        save_checkpoint(\n",
    "            epoch_checkpoint_path,\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            epoch,\n",
    "            len(train_loader),\n",
    "            train_loss,\n",
    "            val_loss,\n",
    "            CONFIG\n",
    "        )\n",
    "        print(f\"‚úì Epoch {epoch} checkpoint saved\")\n",
    "        \n",
    "        # Cleanup old checkpoints\n",
    "        cleanup_old_checkpoints(CONFIG['checkpoint_dir'], CONFIG['max_checkpoints'])\n",
    "        \n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚ö†Ô∏è Training interrupted by user\")\n",
    "    print(\"Saving emergency checkpoint...\")\n",
    "    emergency_path = '/kaggle/working/emergency_checkpoint.pt'\n",
    "    save_checkpoint(\n",
    "        emergency_path,\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        epoch,\n",
    "        step,\n",
    "        train_loss,\n",
    "        config=CONFIG\n",
    "    )\n",
    "    print(f\"‚úì Emergency checkpoint saved to {emergency_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n\\n‚ùå Training failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history_path = '/kaggle/working/training_history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for entry in training_history:\n",
    "    print(f\"Epoch {entry['epoch']}: \"\n",
    "          f\"Train Loss={entry['train_loss']:.4f}, \"\n",
    "          f\"Val Loss={entry['val_loss']:.4f}, \"\n",
    "          f\"Perplexity={entry['val_perplexity']:.2f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚úì Training history saved to {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Text Generation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100, temperature=0.8, num_return_sequences=1):\n",
    "    \"\"\"Generate text from a prompt\"\"\"\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return [tokenizer.decode(seq, skip_special_tokens=True) for seq in output]\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"The future of artificial intelligence\",\n",
    "    \"In a world where technology\",\n",
    "    \"Scientists have discovered\",\n",
    "    \"Once upon a time in a distant land\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEXT GENERATION EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n{'‚îÄ'*60}\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"{'‚îÄ'*60}\")\n",
    "    try:\n",
    "        generated = generate_text(prompt, max_length=150, temperature=0.8)\n",
    "        print(generated[0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating text: {e}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in HuggingFace format\n",
    "final_model_dir = '/kaggle/working/final_model'\n",
    "print(f\"\\nSaving final model to {final_model_dir}...\")\n",
    "\n",
    "try:\n",
    "    model.save_pretrained(final_model_dir)\n",
    "    tokenizer.save_pretrained(final_model_dir)\n",
    "    \n",
    "    # Save training config\n",
    "    config_path = os.path.join(final_model_dir, 'training_config.json')\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(CONFIG, f, indent=2)\n",
    "    \n",
    "    print(\"‚úì Model saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Output Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all output files\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUTPUT FILES\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìÅ /kaggle/working/\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ best_model.pt (best checkpoint based on validation loss)\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ training_history.json (training metrics)\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ checkpoints/ (training checkpoints)\")\n",
    "\n",
    "# List checkpoints\n",
    "checkpoints = get_checkpoint_list(CONFIG['checkpoint_dir'])\n",
    "if checkpoints:\n",
    "    for ckpt in checkpoints:\n",
    "        size_mb = os.path.getsize(ckpt) / 1e6\n",
    "        print(f\"  ‚îÇ   ‚îú‚îÄ‚îÄ {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(\"  ‚îî‚îÄ‚îÄ final_model/ (HuggingFace format)\")\n",
    "print(\"      ‚îú‚îÄ‚îÄ pytorch_model.bin\")\n",
    "print(\"      ‚îú‚îÄ‚îÄ config.json\")\n",
    "print(\"      ‚îú‚îÄ‚îÄ training_config.json\")\n",
    "print(\"      ‚îî‚îÄ‚îÄ tokenizer files\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate total size\n",
    "total_size = 0\n",
    "for root, dirs, files in os.walk('/kaggle/working'):\n",
    "    for file in files:\n",
    "        filepath = os.path.join(root, file)\n",
    "        total_size += os.path.getsize(filepath)\n",
    "\n",
    "print(f\"\\nTotal output size: {total_size / 1e6:.2f} MB\")\n",
    "print(\"\\n‚úì Training complete! Download the files from the Output tab.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
