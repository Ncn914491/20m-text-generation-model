{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20M Parameter Text Generation Model - Kaggle Training (Clean)\n",
    "## Train from Scratch - No Checkpoint Complications\n",
    "\n",
    "This notebook trains a transformer model on Kaggle from scratch with simplified checkpointing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (Kaggle already has most packages pre-installed)\n",
    "# Only install what's missing or needs updating\n",
    "!pip install -q --upgrade transformers datasets tokenizers\n",
    "!pip install -q --no-deps sentencepiece\n",
    "\n",
    "print(\"✓ Packages installed/updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: No GPU detected! Training will be very slow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 5e-4,\n",
    "    'epochs': 3,\n",
    "    'warmup_steps': 500,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'save_steps': 1000,\n",
    "    'eval_steps': 500,\n",
    "    'max_length': 512,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture (10M parameters)\n",
    "model_config = GPT2Config(\n",
    "    vocab_size=50257,\n",
    "    n_positions=512,\n",
    "    n_embd=256,\n",
    "    n_layer=8,\n",
    "    n_head=8,\n",
    "    n_inner=1024,\n",
    "    activation_function='gelu_new',\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    attn_pdrop=0.1,\n",
    ")\n",
    "\n",
    "# Initialize model from scratch\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1e6:.2f} MB (FP32)\")\n",
    "print(\"\\n✓ Starting fresh training from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset('wikitext', 'wikitext-103-v1')\n",
    "\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=CONFIG['max_length'],\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = dataset['train'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "tokenized_val = dataset['validation'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['validation'].column_names\n",
    ")\n",
    "\n",
    "tokenized_train.set_format('torch')\n",
    "tokenized_val.set_format('torch')\n",
    "\n",
    "print(\"✓ Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    tokenized_train,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_val,\n",
    "    batch_size=CONFIG['batch_size']\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "total_steps = len(train_loader) * CONFIG['epochs'] // CONFIG['gradient_accumulation_steps']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=CONFIG['warmup_steps'],\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {CONFIG['warmup_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=input_ids\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss / CONFIG['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % CONFIG['gradient_accumulation_steps'] == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * CONFIG['gradient_accumulation_steps']\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item() * CONFIG['gradient_accumulation_steps'],\n",
    "            'lr': scheduler.get_last_lr()[0]\n",
    "        })\n",
    "        \n",
    "        # Save checkpoint periodically (PyTorch format only)\n",
    "        if (step + 1) % CONFIG['save_steps'] == 0:\n",
    "            checkpoint_path = f'/kaggle/working/checkpoint_epoch{epoch}_step{step+1}.pt'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'step': step + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': total_loss / (step + 1),\n",
    "            }, checkpoint_path)\n",
    "            print(f\"\\n✓ Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            total_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return avg_loss, perplexity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "training_history = []\n",
    "\n",
    "for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch}/{CONFIG['epochs']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        epoch\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_perplexity = evaluate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val Perplexity: {val_perplexity:.2f}\")\n",
    "    \n",
    "    training_history.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'val_perplexity': val_perplexity\n",
    "    })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_perplexity': val_perplexity,\n",
    "        }, '/kaggle/working/best_model.pt')\n",
    "        print(\"✓ Saved best model\")\n",
    "    \n",
    "    # Save epoch checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'val_perplexity': val_perplexity,\n",
    "    }, f'/kaggle/working/checkpoint_epoch{epoch}.pt')\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "with open('/kaggle/working/training_history.json', 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "print(\"Training history saved!\")\n",
    "print(\"\\nFinal Results:\")\n",
    "for entry in training_history:\n",
    "    print(f\"Epoch {entry['epoch']}: Train Loss={entry['train_loss']:.4f}, \"\n",
    "          f\"Val Loss={entry['val_loss']:.4f}, Perplexity={entry['val_perplexity']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Text Generation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100, temperature=0.8):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test generation\n",
    "test_prompts = [\n",
    "    \"The future of artificial intelligence\",\n",
    "    \"In a world where technology\",\n",
    "    \"Scientists have discovered\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Text Generation Examples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 60)\n",
    "    generated = generate_text(prompt, max_length=150)\n",
    "    print(generated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in HuggingFace format\n",
    "model.save_pretrained('/kaggle/working/final_model')\n",
    "tokenizer.save_pretrained('/kaggle/working/final_model')\n",
    "\n",
    "print(\"✓ Model saved in HuggingFace format\")\n",
    "print(\"\\nOutput files:\")\n",
    "print(\"  - best_model.pt (best checkpoint)\")\n",
    "print(\"  - checkpoint_epoch*.pt (epoch checkpoints)\")\n",
    "print(\"  - training_history.json (training metrics)\")\n",
    "print(\"  - final_model/ (HuggingFace format)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
