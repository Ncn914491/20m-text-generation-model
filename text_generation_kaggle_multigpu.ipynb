{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation Model - Multi-GPU Kaggle Training\n",
    "## 10M Parameter GPT-2 with 2xT4 GPU Support\n",
    "\n",
    "**Features:**\n",
    "- ‚ö° Multi-GPU training (2xT4 on Kaggle)\n",
    "- üîÑ Automatic checkpoint management (keeps 4 most recent)\n",
    "- üõ°Ô∏è Robust error handling with OOM recovery\n",
    "- üíæ Smart memory optimization\n",
    "- üìä Progress tracking and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Fix Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix protobuf warnings FIRST (before any other imports)\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Suppress TensorFlow and protobuf warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "# Suppress Python warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Running on Kaggle: {'/kaggle/working' in sys.path or 'KAGGLE_KERNEL_RUN_TYPE' in os.environ}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix protobuf version conflict (common Kaggle issue)\n",
    "!pip uninstall -y protobuf 2>/dev/null\n",
    "!pip install -q protobuf==3.20.3\n",
    "print(\"‚úì Protobuf fixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parallel import DataParallel\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import math\n",
    "\n",
    "print(\"‚úì All imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "try:\n",
    "    import transformers\n",
    "    import datasets as ds\n",
    "    print(f\"Transformers version: {transformers.__version__}\")\n",
    "    print(f\"Datasets version: {ds.__version__}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-GPU Detection and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect all available GPUs\n",
    "if torch.cuda.is_available():\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GPU CONFIGURATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Number of GPUs available: {n_gpus}\")\n",
    "    \n",
    "    for i in range(n_gpus):\n",
    "        print(f\"\\nGPU {i}:\")\n",
    "        print(f\"  Name: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"  Compute Capability: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}\")\n",
    "    \n",
    "    print(f\"\\nCUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "    \n",
    "    # Enable optimizations\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"\\n‚úì TF32 and cuDNN optimizations enabled\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda:0')\n",
    "    use_multi_gpu = n_gpus > 1\n",
    "    \n",
    "    if use_multi_gpu:\n",
    "        print(f\"\\n‚ö° MULTI-GPU MODE: Will use {n_gpus} GPUs with DataParallel\")\n",
    "        print(f\"   Effective batch size will be multiplied by {n_gpus}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Single GPU mode (only 1 GPU detected)\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå ERROR: No GPU detected!\")\n",
    "    print(\"Please enable GPU in Kaggle notebook settings:\")\n",
    "    print(\"  Settings ‚Üí Accelerator ‚Üí GPU T4 x2\")\n",
    "    device = torch.device('cpu')\n",
    "    use_multi_gpu = False\n",
    "    n_gpus = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration (Multi-GPU Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration - optimized for 2xT4 GPUs\n",
    "CONFIG = {\n",
    "    # Model architecture\n",
    "    'vocab_size': 50257,\n",
    "    'n_positions': 512,\n",
    "    'n_embd': 256,\n",
    "    'n_layer': 8,\n",
    "    'n_head': 8,\n",
    "    'n_inner': 1024,\n",
    "    \n",
    "    # Training hyperparameters (adjusted for multi-GPU)\n",
    "    'batch_size': 16 if use_multi_gpu else 8,  # Per-GPU batch size\n",
    "    'gradient_accumulation_steps': 4 if use_multi_gpu else 8,\n",
    "    'learning_rate': 5e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'epochs': 3,\n",
    "    'warmup_steps': 500,\n",
    "    'max_length': 512,\n",
    "    \n",
    "    # Checkpointing\n",
    "    'save_steps': 1000,\n",
    "    'eval_steps': 500,\n",
    "    'max_checkpoints': 4,\n",
    "    'checkpoint_dir': '/kaggle/working/checkpoints',\n",
    "    \n",
    "    # Dataset\n",
    "    'dataset_name': 'wikitext',\n",
    "    'dataset_config': 'wikitext-103-v1',\n",
    "    \n",
    "    # Multi-GPU settings\n",
    "    'use_multi_gpu': use_multi_gpu,\n",
    "    'n_gpus': n_gpus,\n",
    "    \n",
    "    # Resume training\n",
    "    'resume_from_checkpoint': None,\n",
    "}\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "# Calculate effective batch size\n",
    "effective_batch_size = CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']\n",
    "if use_multi_gpu:\n",
    "    effective_batch_size *= n_gpus\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model Parameters: ~10M\")\n",
    "print(f\"\\nMulti-GPU Settings:\")\n",
    "print(f\"  Number of GPUs: {n_gpus}\")\n",
    "print(f\"  Multi-GPU Mode: {'Enabled' if use_multi_gpu else 'Disabled'}\")\n",
    "print(f\"\\nBatch Configuration:\")\n",
    "print(f\"  Per-GPU Batch Size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Gradient Accumulation: {CONFIG['gradient_accumulation_steps']}\")\n",
    "if use_multi_gpu:\n",
    "    print(f\"  Total Batch per Step: {CONFIG['batch_size'] * n_gpus}\")\n",
    "print(f\"  Effective Batch Size: {effective_batch_size}\")\n",
    "print(f\"\\nTraining Settings:\")\n",
    "print(f\"  Learning Rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"  Max Checkpoints: {CONFIG['max_checkpoints']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Checkpoint Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_list(checkpoint_dir):\n",
    "    \"\"\"Get sorted list of checkpoint files\"\"\"\n",
    "    checkpoints = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_*.pt'))\n",
    "    checkpoints.sort(key=os.path.getmtime, reverse=True)\n",
    "    return checkpoints\n",
    "\n",
    "def cleanup_old_checkpoints(checkpoint_dir, max_keep=4):\n",
    "    \"\"\"Keep only the most recent N checkpoints\"\"\"\n",
    "    checkpoints = get_checkpoint_list(checkpoint_dir)\n",
    "    \n",
    "    if len(checkpoints) > max_keep:\n",
    "        to_delete = checkpoints[max_keep:]\n",
    "        for ckpt in to_delete:\n",
    "            try:\n",
    "                os.remove(ckpt)\n",
    "                print(f\"  Deleted old checkpoint: {os.path.basename(ckpt)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not delete {ckpt}: {e}\")\n",
    "\n",
    "def save_checkpoint(filepath, model, optimizer, scheduler, epoch, step, train_loss, val_loss=None, config=None):\n",
    "    \"\"\"Save training checkpoint (handles DataParallel)\"\"\"\n",
    "    try:\n",
    "        # Get model state dict (unwrap DataParallel if needed)\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model_state = model.module.state_dict()\n",
    "        else:\n",
    "            model_state = model.state_dict()\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'step': step,\n",
    "            'model_state_dict': model_state,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'config': config,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, filepath)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving checkpoint: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer=None, scheduler=None):\n",
    "    \"\"\"Load training checkpoint (handles DataParallel)\"\"\"\n",
    "    try:\n",
    "        checkpoint = torch.load(filepath, map_location='cpu')\n",
    "        \n",
    "        # Load model state (handle DataParallel)\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        return {\n",
    "            'epoch': checkpoint.get('epoch', 0),\n",
    "            'step': checkpoint.get('step', 0),\n",
    "            'train_loss': checkpoint.get('train_loss', None),\n",
    "            'val_loss': checkpoint.get('val_loss', None),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úì Checkpoint management functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Initialization with Multi-GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model configuration\n",
    "model_config = GPT2Config(\n",
    "    vocab_size=CONFIG['vocab_size'],\n",
    "    n_positions=CONFIG['n_positions'],\n",
    "    n_embd=CONFIG['n_embd'],\n",
    "    n_layer=CONFIG['n_layer'],\n",
    "    n_head=CONFIG['n_head'],\n",
    "    n_inner=CONFIG['n_inner'],\n",
    "    activation_function='gelu_new',\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    attn_pdrop=0.1,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Vocabulary Size: {model_config.vocab_size:,}\")\n",
    "print(f\"Max Sequence Length: {model_config.n_positions}\")\n",
    "print(f\"Embedding Dimension: {model_config.n_embd}\")\n",
    "print(f\"Number of Layers: {model_config.n_layer}\")\n",
    "print(f\"Number of Attention Heads: {model_config.n_head}\")\n",
    "print(f\"FFN Inner Dimension: {model_config.n_inner}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(\"\\nInitializing model...\")\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "\n",
    "# Move to GPU and wrap with DataParallel if multiple GPUs\n",
    "model = model.to(device)\n",
    "\n",
    "if use_multi_gpu:\n",
    "    print(f\"\\n‚ö° Wrapping model with DataParallel for {n_gpus} GPUs...\")\n",
    "    model = nn.DataParallel(model, device_ids=list(range(n_gpus)))\n",
    "    print(f\"‚úì Model distributed across GPUs: {list(range(n_gpus))}\")\n",
    "\n",
    "# Count parameters\n",
    "if isinstance(model, nn.DataParallel):\n",
    "    total_params = sum(p.numel() for p in model.module.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.module.parameters() if p.requires_grad)\n",
    "else:\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n‚úì Model initialized successfully\")\n",
    "print(f\"  Total Parameters: {total_params:,}\")\n",
    "print(f\"  Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"  Model Size (FP32): {total_params * 4 / 1e6:.2f} MB\")\n",
    "if use_multi_gpu:\n",
    "    print(f\"  Per-GPU Memory: ~{total_params * 4 / 1e6 / n_gpus:.2f} MB (replicated on each GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"\\nLoading tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"‚úì Tokenizer loaded (vocab size: {len(tokenizer):,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"\\nLoading dataset...\")\n",
    "try:\n",
    "    dataset = load_dataset(CONFIG['dataset_name'], CONFIG['dataset_config'])\n",
    "    print(f\"‚úì Dataset loaded successfully\")\n",
    "    print(f\"  Train samples: {len(dataset['train']):,}\")\n",
    "    print(f\"  Validation samples: {len(dataset['validation']):,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=CONFIG['max_length'],\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "print(\"\\nTokenizing datasets...\")\n",
    "tokenized_train = dataset['train'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    "    desc=\"Tokenizing train set\"\n",
    ")\n",
    "\n",
    "tokenized_val = dataset['validation'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['validation'].column_names,\n",
    "    desc=\"Tokenizing validation set\"\n",
    ")\n",
    "\n",
    "tokenized_train.set_format('torch')\n",
    "tokenized_val.set_format('torch')\n",
    "print(\"‚úì Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders (adjusted for multi-GPU)\n",
    "print(\"\\nCreating dataloaders...\")\n",
    "train_loader = DataLoader(\n",
    "    tokenized_train,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_val,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"‚úì DataLoaders created\")\n",
    "print(f\"  Train batches: {len(train_loader):,}\")\n",
    "print(f\"  Validation batches: {len(val_loader):,}\")\n",
    "if use_multi_gpu:\n",
    "    print(f\"  Samples per step: {CONFIG['batch_size'] * n_gpus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimizer and Scheduler Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Calculate total training steps\n",
    "total_steps = (len(train_loader) * CONFIG['epochs']) // CONFIG['gradient_accumulation_steps']\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=CONFIG['warmup_steps'],\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SETUP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Training Steps: {total_steps:,}\")\n",
    "print(f\"Warmup Steps: {CONFIG['warmup_steps']:,}\")\n",
    "print(f\"Initial Learning Rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Weight Decay: {CONFIG['weight_decay']}\")\n",
    "if use_multi_gpu:\n",
    "    print(f\"\\n‚ö° Multi-GPU Training:\")\n",
    "    print(f\"  Training will be {n_gpus}x faster (approximately)\")\n",
    "    print(f\"  Each GPU processes {CONFIG['batch_size']} samples\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resume from Checkpoint (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training from checkpoint if specified\n",
    "start_epoch = 1\n",
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "if CONFIG['resume_from_checkpoint'] and os.path.exists(CONFIG['resume_from_checkpoint']):\n",
    "    print(f\"\\nResuming from checkpoint: {CONFIG['resume_from_checkpoint']}\")\n",
    "    metadata = load_checkpoint(\n",
    "        CONFIG['resume_from_checkpoint'],\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler\n",
    "    )\n",
    "    \n",
    "    if metadata:\n",
    "        start_epoch = metadata['epoch'] + 1\n",
    "        global_step = metadata['step']\n",
    "        if metadata['val_loss']:\n",
    "            best_val_loss = metadata['val_loss']\n",
    "        print(f\"‚úì Resumed from epoch {metadata['epoch']}, step {global_step}\")\n",
    "else:\n",
    "    print(\"\\n‚úì Starting training from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Functions (Multi-GPU Compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, device, epoch, config):\n",
    "    \"\"\"Train for one epoch with multi-GPU support\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        try:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Forward pass (DataParallel handles distribution automatically)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            \n",
    "            # Loss is automatically averaged across GPUs by DataParallel\n",
    "            loss = outputs.loss / config['gradient_accumulation_steps']\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights after accumulation\n",
    "            if (step + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Track loss\n",
    "            total_loss += loss.item() * config['gradient_accumulation_steps']\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{loss.item() * config['gradient_accumulation_steps']:.4f}\",\n",
    "                'lr': f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
    "                'gpus': f\"{config['n_gpus']}\" if config['use_multi_gpu'] else '1'\n",
    "            })\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if (step + 1) % config['save_steps'] == 0:\n",
    "                checkpoint_path = os.path.join(\n",
    "                    config['checkpoint_dir'],\n",
    "                    f\"checkpoint_epoch{epoch}_step{step+1}.pt\"\n",
    "                )\n",
    "                \n",
    "                if save_checkpoint(\n",
    "                    checkpoint_path,\n",
    "                    model,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    epoch,\n",
    "                    step + 1,\n",
    "                    total_loss / (step + 1),\n",
    "                    config=config\n",
    "                ):\n",
    "                    print(f\"\\n‚úì Checkpoint saved: {os.path.basename(checkpoint_path)}\")\n",
    "                    cleanup_old_checkpoints(config['checkpoint_dir'], config['max_checkpoints'])\n",
    "        \n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e):\n",
    "                print(f\"\\n‚ö†Ô∏è OOM Error at step {step}. Clearing cache...\")\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate the model (multi-GPU compatible)\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=input_ids\n",
    "                )\n",
    "                total_loss += outputs.loss.item()\n",
    "            \n",
    "            except RuntimeError as e:\n",
    "                if 'out of memory' in str(e):\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    perplexity = math.exp(avg_loss) if avg_loss < 100 else float('inf')\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "print(\"‚úì Training functions defined (multi-GPU compatible)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "training_history = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training from epoch {start_epoch} to {CONFIG['epochs']}\")\n",
    "if use_multi_gpu:\n",
    "    print(f\"‚ö° Using {n_gpus} GPUs in parallel\")\n",
    "    print(f\"‚ö° Effective speedup: ~{n_gpus}x\")\n",
    "print(f\"Checkpoints: {CONFIG['checkpoint_dir']}\")\n",
    "print(f\"Keeping {CONFIG['max_checkpoints']} most recent checkpoints\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, CONFIG['epochs'] + 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EPOCH {epoch}/{CONFIG['epochs']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            device,\n",
    "            epoch,\n",
    "            CONFIG\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_perplexity = evaluate(model, val_loader, device)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch} Results:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Perplexity: {val_perplexity:.2f}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Save history\n",
    "        training_history.append({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': float(train_loss),\n",
    "            'val_loss': float(val_loss),\n",
    "            'val_perplexity': float(val_perplexity),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = '/kaggle/working/best_model.pt'\n",
    "            \n",
    "            if save_checkpoint(\n",
    "                best_model_path,\n",
    "                model,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                epoch,\n",
    "                len(train_loader),\n",
    "                train_loss,\n",
    "                val_loss,\n",
    "                CONFIG\n",
    "            ):\n",
    "                print(f\"\\n‚úì New best model saved! (val_loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Save epoch checkpoint\n",
    "        epoch_checkpoint_path = os.path.join(\n",
    "            CONFIG['checkpoint_dir'],\n",
    "            f\"checkpoint_epoch{epoch}_final.pt\"\n",
    "        )\n",
    "        \n",
    "        save_checkpoint(\n",
    "            epoch_checkpoint_path,\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            epoch,\n",
    "            len(train_loader),\n",
    "            train_loss,\n",
    "            val_loss,\n",
    "            CONFIG\n",
    "        )\n",
    "        print(f\"‚úì Epoch {epoch} checkpoint saved\")\n",
    "        \n",
    "        # Cleanup old checkpoints\n",
    "        cleanup_old_checkpoints(CONFIG['checkpoint_dir'], CONFIG['max_checkpoints'])\n",
    "        \n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    if use_multi_gpu:\n",
    "        print(f\"‚ö° Trained using {n_gpus} GPUs\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚ö†Ô∏è Training interrupted by user\")\n",
    "    emergency_path = '/kaggle/working/emergency_checkpoint.pt'\n",
    "    save_checkpoint(emergency_path, model, optimizer, scheduler, epoch, step, train_loss, config=CONFIG)\n",
    "    print(f\"‚úì Emergency checkpoint saved\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n\\n‚ùå Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history_path = '/kaggle/working/training_history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for entry in training_history:\n",
    "    print(f\"Epoch {entry['epoch']}: \"\n",
    "          f\"Train={entry['train_loss']:.4f}, \"\n",
    "          f\"Val={entry['val_loss']:.4f}, \"\n",
    "          f\"PPL={entry['val_perplexity']:.2f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚úì Training history saved to {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Text Generation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100, temperature=0.8, num_return_sequences=1):\n",
    "    \"\"\"Generate text (uses only GPU 0 for generation)\"\"\"\n",
    "    # For generation, use the base model (not DataParallel)\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        gen_model = model.module\n",
    "    else:\n",
    "        gen_model = model\n",
    "    \n",
    "    gen_model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = gen_model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return [tokenizer.decode(seq, skip_special_tokens=True) for seq in output]\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"The future of artificial intelligence\",\n",
    "    \"In a world where technology\",\n",
    "    \"Scientists have discovered\",\n",
    "    \"Once upon a time\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEXT GENERATION EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n{'‚îÄ'*60}\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"{'‚îÄ'*60}\")\n",
    "    try:\n",
    "        generated = generate_text(prompt, max_length=150, temperature=0.8)\n",
    "        print(generated[0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in HuggingFace format (unwrap DataParallel)\n",
    "final_model_dir = '/kaggle/working/final_model'\n",
    "print(f\"\\nSaving final model to {final_model_dir}...\")\n",
    "\n",
    "try:\n",
    "    # Get the base model (unwrap DataParallel if needed)\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        save_model = model.module\n",
    "    else:\n",
    "        save_model = model\n",
    "    \n",
    "    save_model.save_pretrained(final_model_dir)\n",
    "    tokenizer.save_pretrained(final_model_dir)\n",
    "    \n",
    "    # Save training config\n",
    "    config_path = os.path.join(final_model_dir, 'training_config.json')\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(CONFIG, f, indent=2)\n",
    "    \n",
    "    print(\"‚úì Model saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Output Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all output files\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUTPUT FILES\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìÅ /kaggle/working/\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ best_model.pt (best checkpoint)\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ training_history.json\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ checkpoints/\")\n",
    "\n",
    "checkpoints = get_checkpoint_list(CONFIG['checkpoint_dir'])\n",
    "if checkpoints:\n",
    "    for ckpt in checkpoints:\n",
    "        size_mb = os.path.getsize(ckpt) / 1e6\n",
    "        print(f\"  ‚îÇ   ‚îú‚îÄ‚îÄ {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(\"  ‚îî‚îÄ‚îÄ final_model/ (HuggingFace format)\")\n",
    "print(\"      ‚îú‚îÄ‚îÄ pytorch_model.bin\")\n",
    "print(\"      ‚îú‚îÄ‚îÄ config.json\")\n",
    "print(\"      ‚îú‚îÄ‚îÄ training_config.json\")\n",
    "print(\"      ‚îî‚îÄ‚îÄ tokenizer files\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if use_multi_gpu:\n",
    "    print(f\"\\n‚ö° Training completed using {n_gpus} GPUs\")\n",
    "    print(f\"‚ö° Approximate speedup: {n_gpus}x vs single GPU\")\n",
    "\n",
    "print(\"\\n‚úì Training complete! Download files from the Output tab.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
